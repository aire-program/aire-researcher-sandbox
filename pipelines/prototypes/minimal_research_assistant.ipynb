{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958d8e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, subprocess\n",
    "if \"google.colab\" in sys.modules:\n",
    "    subprocess.run([\"pip\", \"install\", \"-q\", \"pandas\", \"numpy\", \"scikit-learn\", \"requests\", \"pydantic\", \"jsonschema\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110e176b",
   "metadata": {},
   "source": [
    "# Minimal Research Assistant Prototype\n",
    "\n",
    "**What:** Combine TF-IDF retrieval with templated responses to simulate a lightweight research Q&A assistant.\n",
    "\n",
    "**Why:** This provides a safe rehearsal of retrieval-augmented patterns without external APIs or sensitive data.\n",
    "\n",
    "**How:** Run the install cell in Colab if needed, make sure the index exists or let the notebook create it, then execute cells. Retrieval means pulling the most relevant abstracts; the template simply weaves them into a short answer.\n",
    "\n",
    "**You will learn:** How retrieval results influence responses and how to iterate on prompts in a controlled setting.\n",
    "\n",
    "By the end of this notebook, you will have completed the listed steps and produced the outputs described in the success criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e872cde",
   "metadata": {},
   "source": [
    "### Success criteria\n",
    "- You combined retrieval with templated answers.\n",
    "- You saw responses grounded in retrieved abstracts.\n",
    "- You identified how retrieval impacts answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cfdc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "articles = pd.read_csv(DATA_DIR / \"sample_texts\" / \"articles_sample.csv\")\n",
    "index_path = DATA_DIR / \"vector_index.pkl\"\n",
    "\n",
    "if not index_path.exists():\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    tfidf_matrix = vectorizer.fit_transform(articles[\"abstract\"].fillna(\"\"))\n",
    "    with open(index_path, \"wb\") as handle:\n",
    "        pickle.dump({\"vectorizer\": vectorizer, \"tfidf_matrix\": tfidf_matrix}, handle)\n",
    "else:\n",
    "    with open(index_path, \"rb\") as handle:\n",
    "        payload = pickle.load(handle)\n",
    "    vectorizer = payload[\"vectorizer\"]\n",
    "    tfidf_matrix = payload[\"tfidf_matrix\"]\n",
    "\n",
    "\n",
    "def answer(query: str) -> str:\n",
    "    scores = cosine_similarity(vectorizer.transform([query]), tfidf_matrix).flatten()\n",
    "    top_idx = scores.argmax()\n",
    "    article = articles.iloc[top_idx]\n",
    "    return f\"Based on {article['title']}, consider: {article['abstract'][:150]}...\"\n",
    "\n",
    "for q in [\"How to document AI methods?\", \"Ways to improve study reproducibility?\"]:\n",
    "    print(q)\n",
    "    print(answer(q))\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82b318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "articles = pd.read_csv(DATA_DIR / \"sample_texts\" / \"articles_sample.csv\")\n",
    "with open(DATA_DIR / \"vector_index.pkl\", \"rb\") as handle:\n",
    "    payload = pickle.load(handle)\n",
    "vectorizer = payload[\"vectorizer\"]\n",
    "tfidf_matrix = payload[\"tfidf_matrix\"]\n",
    "\n",
    "\n",
    "def answer(query: str) -> str:\n",
    "    scores = cosine_similarity(vectorizer.transform([query]), tfidf_matrix).flatten()\n",
    "    top_idx = scores.argmax()\n",
    "    article = articles.iloc[top_idx]\n",
    "    return f\"Based on {article['title']}, consider: {article['abstract'][:150]}...\"\n",
    "\n",
    "for q in [\"How to document AI methods?\", \"Ways to improve study reproducibility?\"]:\n",
    "    print(q)\n",
    "    print(answer(q))\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390f94ed",
   "metadata": {},
   "source": [
    "### If you get stuck / What to try next\n",
    "\n",
    "If you get stuck: rerun the index build cells and ensure dependencies are installed. What to try next: compare with the semantic search demo in pipelines/prototypes/semantic_search_demo.ipynb or explore the Streamlit RAG Workbench."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
