{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b848484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, subprocess\n",
    "if \"google.colab\" in sys.modules:\n",
    "    subprocess.run([\"pip\", \"install\", \"-q\", \"pandas\", \"numpy\", \"scikit-learn\", \"requests\", \"pydantic\", \"jsonschema\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03997e8",
   "metadata": {},
   "source": [
    "# Build a TF-IDF Retrieval Index\n",
    "\n",
    "**What:** Fit a TF-IDF vectorizer on synthetic abstracts and save the index for later querying.\n",
    "\n",
    "**Why:** Retrieval-augmented workflows rely on a consistent index; this practice run is safe and repeatable.\n",
    "\n",
    "**How:** Run the install cell in Colab if needed, confirm data generation, then execute cells. TF-IDF turns text into numbers; cosine similarity (used later) measures how close two vectors are.\n",
    "\n",
    "**You will learn:** How to build and persist a text index suitable for downstream querying and evaluation.\n",
    "\n",
    "By the end of this notebook, you will have completed the listed steps and produced the outputs described in the success criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9251a704",
   "metadata": {},
   "source": [
    "### Success criteria\n",
    "- You fit a TF-IDF vectorizer on abstracts.\n",
    "- You saved an index file (vector_index.pkl).\n",
    "- You know the document and feature counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cfdc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def find_data_dir() -> Path:\n",
    "    candidates = [Path.cwd() / \"data\", Path.cwd().parent / \"data\", Path.cwd().parent.parent / \"data\"]\n",
    "    for candidate in candidates:\n",
    "        if (candidate / \"sample_texts\" / \"articles_sample.csv\").exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError(\"data directory not found. Run scripts/generate_synthetic_data.py.\")\n",
    "\n",
    "DATA_DIR = find_data_dir()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779d631c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "articles = pd.read_csv(DATA_DIR / \"sample_texts\" / \"articles_sample.csv\")\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "tfidf_matrix = vectorizer.fit_transform(articles[\"abstract\"].fillna(\"\"))\n",
    "\n",
    "index_payload = {\n",
    "    \"vectorizer\": vectorizer,\n",
    "    \"tfidf_matrix\": tfidf_matrix,\n",
    "    \"article_ids\": articles[\"article_id\"].tolist(),\n",
    "}\n",
    "index_path = DATA_DIR / \"vector_index.pkl\"\n",
    "with open(index_path, \"wb\") as handle:\n",
    "    pickle.dump(index_payload, handle)\n",
    "print(f\"Index saved to {index_path} with shape {tfidf_matrix.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce7329a",
   "metadata": {},
   "source": [
    "### If you get stuck / What to try next\n",
    "\n",
    "If you get stuck: confirm data generation, rerun installs, and check that TF-IDF parameters match available RAM. What to try next: query the index in pipelines/rag/rag_query.ipynb and compare queries in pipelines/rag/rag_evaluation.ipynb."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
