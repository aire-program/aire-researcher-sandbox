{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b848484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, subprocess\n",
    "if \"google.colab\" in sys.modules:\n",
    "    subprocess.run([\"pip\", \"install\", \"-q\", \"pandas\", \"numpy\", \"scikit-learn\", \"requests\", \"pydantic\", \"jsonschema\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03997e8",
   "metadata": {},
   "source": [
    "# Build a TF-IDF Retrieval Index\n",
    "\n",
    "**What**: Create and persist a TF-IDF vectorizer and index from synthetic abstracts.\n",
    "\n",
    "**Why**: A pre-computed index allows for fast and consistent retrieval in downstream applications (like RAG).\n",
    "\n",
    "**How**:\n",
    "1. **Load synthetic abstracts**.\n",
    "2. **Fit a TF-IDF vectorizer** to the text corpus.\n",
    "3. **Save the index** and vectorizer to disk for later use.\n",
    "\n",
    "**Key Concept**: **Vectorization** is the process of converting text into numerical vectors that computers can process and compare.\n",
    "\n",
    "By the end of this notebook, you will have completed the listed steps and produced the outputs described in the success criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9251a704",
   "metadata": {},
   "source": [
    "### Success criteria\n",
    "- You fit a TF-IDF vectorizer on abstracts.\n",
    "- You saved an index file (vector_index.pkl).\n",
    "- You know the document and feature counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cfdc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def find_data_dir() -> Path:\n",
    "    candidates = [Path.cwd() / \"data\", Path.cwd().parent / \"data\", Path.cwd().parent.parent / \"data\"]\n",
    "    for candidate in candidates:\n",
    "        if (candidate / \"sample_texts\" / \"articles_sample.csv\").exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError(\"data directory not found. Run scripts/generate_synthetic_data.py.\")\n",
    "\n",
    "DATA_DIR = find_data_dir()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779d631c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "articles = pd.read_csv(DATA_DIR / \"sample_texts\" / \"articles_sample.csv\")\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "tfidf_matrix = vectorizer.fit_transform(articles[\"abstract\"].fillna(\"\"))\n",
    "\n",
    "index_payload = {\n",
    "    \"vectorizer\": vectorizer,\n",
    "    \"tfidf_matrix\": tfidf_matrix,\n",
    "    \"article_ids\": articles[\"article_id\"].tolist(),\n",
    "}\n",
    "index_path = DATA_DIR / \"vector_index.pkl\"\n",
    "with open(index_path, \"wb\") as handle:\n",
    "    pickle.dump(index_payload, handle)\n",
    "print(f\"Index saved to {index_path} with shape {tfidf_matrix.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce7329a",
   "metadata": {},
   "source": [
    "### If you get stuck / What to try next\n",
    "\n",
    "If you get stuck: confirm data generation, rerun installs, and check that TF-IDF parameters match available RAM. What to try next: query the index in pipelines/rag/rag_query.ipynb and compare queries in pipelines/rag/rag_evaluation.ipynb."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}