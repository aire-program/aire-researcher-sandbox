{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db194b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, subprocess\n",
    "if \"google.colab\" in sys.modules:\n",
    "    subprocess.run([\"pip\", \"install\", \"-q\", \"pandas\", \"numpy\", \"scikit-learn\", \"requests\", \"pydantic\", \"jsonschema\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144351c9",
   "metadata": {},
   "source": [
    "### Success criteria\n",
    "- You evaluated several queries.\n",
    "- You recorded best scores.\n",
    "- You compared which queries perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c411af36",
   "metadata": {},
   "source": [
    "# Evaluate Retrieval Quality\n",
    "\n",
    "**What this notebook does:** Compares maximum similarity scores across a set of example queries against the TF-IDF index.\n",
    "\n",
    "**Why it matters:** Even simple offline evaluation guides prompt design, corpus scope, and indexing parameters before deploying assistants.\n",
    "\n",
    "**How to use it:**\n",
    "1. Build or load the TF-IDF index.\n",
    "2. Add or change queries to stress-test relevance.\n",
    "\n",
    "**Expected outcome:** A small table of queries with best scores to help you reason about retrieval coverage and tuning needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cfdc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "articles = pd.read_csv(DATA_DIR / \"sample_texts\" / \"articles_sample.csv\")\n",
    "index_path = DATA_DIR / \"vector_index.pkl\"\n",
    "\n",
    "if not index_path.exists():\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    tfidf_matrix = vectorizer.fit_transform(articles[\"abstract\"].fillna(\"\"))\n",
    "    with open(index_path, \"wb\") as handle:\n",
    "        pickle.dump({\"vectorizer\": vectorizer, \"tfidf_matrix\": tfidf_matrix}, handle)\n",
    "else:\n",
    "    with open(index_path, \"rb\") as handle:\n",
    "        payload = pickle.load(handle)\n",
    "    vectorizer = payload[\"vectorizer\"]\n",
    "    tfidf_matrix = payload[\"tfidf_matrix\"]\n",
    "\n",
    "\n",
    "queries = [\n",
    "    \"quantitative study design\",\n",
    "    \"community impacts of technology\",\n",
    "    \"statistical methods for small samples\",\n",
    "]\n",
    "results = []\n",
    "for q in queries:\n",
    "    score = cosine_similarity(vectorizer.transform([q]), tfidf_matrix).flatten().max()\n",
    "    results.append({\"query\": q, \"best_score\": float(score)})\n",
    "\n",
    "pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af96e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "articles = pd.read_csv(DATA_DIR / \"sample_texts\" / \"articles_sample.csv\")\n",
    "with open(DATA_DIR / \"vector_index.pkl\", \"rb\") as handle:\n",
    "    payload = pickle.load(handle)\n",
    "vectorizer = payload[\"vectorizer\"]\n",
    "tfidf_matrix = payload[\"tfidf_matrix\"]\n",
    "\n",
    "queries = [\n",
    "    \"quantitative study design\",\n",
    "    \"community impacts of technology\",\n",
    "    \"statistical methods for small samples\",\n",
    "]\n",
    "results = []\n",
    "for q in queries:\n",
    "    score = cosine_similarity(vectorizer.transform([q]), tfidf_matrix).flatten().max()\n",
    "    results.append({\"query\": q, \"best_score\": float(score)})\n",
    "\n",
    "pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361d1014",
   "metadata": {},
   "source": [
    "### If you get stuck / What to try next\n",
    "\n",
    "If you get stuck: verify the index file exists or rerun build_index. What to try next: apply the scoring insights to pipelines/prototypes/minimal_research_assistant.ipynb."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
