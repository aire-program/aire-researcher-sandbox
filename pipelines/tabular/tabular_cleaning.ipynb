{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee319413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    print(\"Detected Google Colab runtime. Installing dependencies...\")\n",
    "    packages = [\"streamlit\", \"pandas\", \"numpy\", \"scikit-learn\", \"requests\"]\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *packages])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a8c372",
   "metadata": {},
   "source": [
    "# Tabular Cleaning\n",
    "\n",
    "Goal: load synthetic experiment results, parse timestamps, and surface potential quality issues.\n",
    "\n",
    "Why it matters: sanity checks protect downstream analyses from malformed or out-of-range values.\n",
    "\n",
    "How to run and adapt: execute after data generation; add domain-specific validation rules that reflect your own experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cfdc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def find_data_dir() -> Path:\n",
    "    candidates = [Path.cwd() / \"data\", Path.cwd().parent / \"data\", Path.cwd().parent.parent / \"data\"]\n",
    "    for candidate in candidates:\n",
    "        if (candidate / \"sample_texts\" / \"articles_sample.csv\").exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError(\"data directory not found. Run scripts/generate_synthetic_data.py.\")\n",
    "\n",
    "DATA_DIR = find_data_dir()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2ecf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "experiments = pd.read_csv(DATA_DIR / \"sample_tabular\" / \"experiments_sample.csv\")\n",
    "experiments[\"timestamp\"] = pd.to_datetime(experiments[\"timestamp\"])\n",
    "\n",
    "print(\"Dataset shape\", experiments.shape)\n",
    "print(\"Missing values\", experiments.isna().sum())\n",
    "experiments.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b8e9fd",
   "metadata": {},
   "source": [
    "## Basic range checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6552bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_out_of_range = (~experiments[\"metric_value\"].between(0, 100)).sum()\n",
    "print(f\"Records outside expected metric range: {metric_out_of_range}\")\n",
    "experiments.describe()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
