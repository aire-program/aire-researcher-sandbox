{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77590a0e",
   "metadata": {
    "id": "cell-8b45ef87"
   },
   "outputs": [],
   "source": [
    "import sys, subprocess, os\n",
    "from pathlib import Path\n",
    "\n",
    "# Colab Setup\n",
    "if \"google.colab\" in sys.modules:\n",
    "    print(\"Running in Google Colab. Installing dependencies...\")\n",
    "    subprocess.run([\"pip\", \"install\", \"-q\", \"pandas\", \"numpy\", \"scikit-learn\", \"requests\", \"pydantic\", \"jsonschema\", \"plotly\", \"tqdm\"])\n",
    "    \n",
    "    # Check for data\n",
    "    if not (Path.cwd() / \"data\").exists():\n",
    "        print(\"Data directory not found. Cloning repository...\")\n",
    "        subprocess.run([\"git\", \"clone\", \"https://github.com/aire-program/aire-researcher-sandbox.git\", \"_repo\"])\n",
    "        \n",
    "        # Move data and scripts to current directory\n",
    "        if (Path(\"_repo/data\").exists()):\n",
    "            print(\"Moving data and scripts...\")\n",
    "            subprocess.run([\"mv\", \"_repo/data\", \".\"])\n",
    "            subprocess.run([\"mv\", \"_repo/scripts\", \".\"])\n",
    "            subprocess.run([\"rm\", \"-rf\", \"_repo\"])\n",
    "        else:\n",
    "            print(\"Warning: Data not found in cloned repo.\")\n",
    "    else:\n",
    "        print(\"Data directory found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb64945",
   "metadata": {
    "id": "cell-76a69c31"
   },
   "source": [
    "# Ingest and Clean Synthetic Articles\n",
    "\n",
    "**What**: Normalize and prepare synthetic article titles and abstracts for downstream analysis.\n",
    "\n",
    "**Why**: Text data often contains noise that can degrade the performance of clustering and retrieval algorithms. Establishing a clean baseline is a critical first step in any NLP pipeline.\n",
    "\n",
    "**How**:\n",
    "1. **Install dependencies** (if running in Colab).\n",
    "2. **Load data** from the synthetic dataset.\n",
    "3. **Apply cleaning functions** to normalize text (lowercase, remove special characters).\n",
    "4. **Verify** the output structure.\n",
    "\n",
    "**Key Concept**: **Normalization** is the process of transforming text into a standard format (e.g., lowercase, no punctuation) to ensure consistency during analysis.\n",
    "\n",
    "By the end of this notebook, you will have completed the listed steps and produced the outputs described in the success criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bd4968",
   "metadata": {
    "id": "cell-8ad75237"
   },
   "source": [
    "### Success criteria\n",
    "- You loaded synthetic articles.\n",
    "- You produced cleaned text and saw the shape of your dataset.\n",
    "- You have a DataFrame ready for clustering or retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cfdc8b",
   "metadata": {
    "id": "cell-c3656d06"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def find_data_dir() -> Path:\n",
    "    candidates = [Path.cwd() / \"data\", Path.cwd().parent / \"data\", Path.cwd().parent.parent / \"data\"]\n",
    "    for candidate in candidates:\n",
    "        if (candidate / \"sample_texts\" / \"articles_sample.csv\").exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError(\"data directory not found. Run scripts/generate_synthetic_data.py.\")\n",
    "\n",
    "DATA_DIR = find_data_dir()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8968ae",
   "metadata": {
    "id": "cell-36301d1a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "articles_path = DATA_DIR / \"sample_texts\" / \"articles_sample.csv\"\n",
    "articles = pd.read_csv(articles_path)\n",
    "print(f\"Loaded {len(articles)} articles from {articles_path}\")\n",
    "articles.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c972f2aa",
   "metadata": {
    "id": "cell-7e035677"
   },
   "source": [
    "## Basic cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f37e3d",
   "metadata": {
    "id": "cell-c5ac2669"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "articles[\"cleaned\"] = articles[\"abstract\"].apply(clean_text)\n",
    "articles[[\"title\", \"cleaned\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8c7c0f",
   "metadata": {
    "id": "cell-e7a957ee"
   },
   "source": [
    "### If you get stuck / What to try next\n",
    "\n",
    "If you get stuck: rerun the first Colab setup cell and ensure `scripts/generate_synthetic_data.py` has been run. What to try next: cluster cleaned abstracts in pipelines/text/clustering_and_topics.ipynb (see docs/colab_index.md for a Colab badge)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
