{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9ca58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIRE Micro Tutor Identity Cell\n",
    "import datetime as dt\n",
    "learner_id = input('Enter your learner ID (email or handle): ').strip()\n",
    "learner_role = input('Enter your learner role (e.g., researcher, educator): ').strip()\n",
    "resource_id = 'notebook:text_cleaning'\n",
    "session_id = f\"{learner_id}-{dt.datetime.utcnow().isoformat()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99321136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIRE Micro Tutor Imports\n",
    "from prompt_tutor import evaluate_prompt\n",
    "from aire_telemetry import log_event\n",
    "from resources_map import RESOURCE_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77590a0e",
   "metadata": {
    "id": "cell-8b45ef87"
   },
   "outputs": [],
   "source": [
    "import sys, subprocess, os\n",
    "from pathlib import Path\n",
    "\n",
    "# Colab Setup\n",
    "if \"google.colab\" in sys.modules:\n",
    "    print(\"Running in Google Colab. Installing dependencies...\")\n",
    "    subprocess.run([\"pip\", \"install\", \"-q\", \"pandas\", \"numpy\", \"scikit-learn\", \"requests\", \"pydantic\", \"jsonschema\", \"plotly\", \"tqdm\"])\n",
    "    \n",
    "    # Check for data\n",
    "    if not (Path.cwd() / \"data\").exists():\n",
    "        print(\"Data directory not found. Cloning repository...\")\n",
    "        subprocess.run([\"git\", \"clone\", \"https://github.com/aire-program/aire-researcher-sandbox.git\", \"_repo\"])\n",
    "        \n",
    "        # Move data and scripts to current directory\n",
    "        if (Path(\"_repo/data\").exists()):\n",
    "            print(\"Moving data and scripts...\")\n",
    "            subprocess.run([\"mv\", \"_repo/data\", \".\"])\n",
    "            subprocess.run([\"mv\", \"_repo/scripts\", \".\"])\n",
    "            subprocess.run([\"rm\", \"-rf\", \"_repo\"])\n",
    "        else:\n",
    "            print(\"Warning: Data not found in cloned repo.\")\n",
    "    else:\n",
    "        print(\"Data directory found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb64945",
   "metadata": {
    "id": "cell-76a69c31"
   },
   "source": [
    "# Ingest and Clean Synthetic Articles\n",
    "\n",
    "**What**: Normalize and prepare synthetic article titles and abstracts for downstream analysis.\n",
    "\n",
    "**Why**: Text data often contains noise that can degrade the performance of clustering and retrieval algorithms. Establishing a clean baseline is a critical first step in any NLP pipeline.\n",
    "\n",
    "**How**:\n",
    "1. **Install dependencies** (if running in Colab).\n",
    "2. **Load data** from the synthetic dataset.\n",
    "3. **Apply cleaning functions** to normalize text (lowercase, remove special characters).\n",
    "4. **Verify** the output structure.\n",
    "\n",
    "**Key Concept**: **Normalization** is the process of transforming text into a standard format (e.g., lowercase, no punctuation) to ensure consistency during analysis.\n",
    "\n",
    "By the end of this notebook, you will have completed the listed steps and produced the outputs described in the success criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bd4968",
   "metadata": {
    "id": "cell-8ad75237"
   },
   "source": [
    "### Success criteria\n",
    "- You loaded synthetic articles.\n",
    "- You produced cleaned text and saw the shape of your dataset.\n",
    "- You have a DataFrame ready for clustering or retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cfdc8b",
   "metadata": {
    "id": "cell-c3656d06"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def find_data_dir() -> Path:\n",
    "    candidates = [Path.cwd() / \"data\", Path.cwd().parent / \"data\", Path.cwd().parent.parent / \"data\"]\n",
    "    for candidate in candidates:\n",
    "        if (candidate / \"sample_texts\" / \"articles_sample.csv\").exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError(\"data directory not found. Run scripts/generate_synthetic_data.py.\")\n",
    "\n",
    "DATA_DIR = find_data_dir()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8968ae",
   "metadata": {
    "id": "cell-36301d1a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "articles_path = DATA_DIR / \"sample_texts\" / \"articles_sample.csv\"\n",
    "articles = pd.read_csv(articles_path)\n",
    "print(f\"Loaded {len(articles)} articles from {articles_path}\")\n",
    "articles.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c972f2aa",
   "metadata": {
    "id": "cell-7e035677"
   },
   "source": [
    "## Basic cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f37e3d",
   "metadata": {
    "id": "cell-c5ac2669"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "articles[\"cleaned\"] = articles[\"abstract\"].apply(clean_text)\n",
    "articles[[\"title\", \"cleaned\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8c7c0f",
   "metadata": {
    "id": "cell-e7a957ee"
   },
   "source": [
    "### If you get stuck / What to try next\n",
    "\n",
    "If you get stuck: rerun the first Colab setup cell and ensure `scripts/generate_synthetic_data.py` has been run. What to try next: cluster cleaned abstracts in pipelines/text/clustering_and_topics.ipynb (see docs/colab_index.md for a Colab badge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c29060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIRE Micro Tutor Prompt + Feedback Cell\n",
    "from aire_llm_client import chat_completion\n",
    "\n",
    "def call_llm(prompt_text: str) -> str:\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': 'You are an AI helper responding concisely.'},\n",
    "        {'role': 'user', 'content': prompt_text},\n",
    "    ]\n",
    "    return chat_completion(messages)\n",
    "\n",
    "prompt_text = input('Enter the prompt you want to test: ').strip()\n",
    "ai_response = call_llm(prompt_text)\n",
    "evaluation = evaluate_prompt(prompt_text, learner_role)\n",
    "\n",
    "clarity = evaluation.get('clarity_score')\n",
    "context = evaluation.get('context_score')\n",
    "constraints = evaluation.get('constraints_score')\n",
    "evaluation_score = evaluation.get('evaluation_score')\n",
    "primary_weakness = evaluation.get('primary_weakness', '')\n",
    "recommended_resource_id = RESOURCE_MAP.get(primary_weakness) or RESOURCE_MAP.get('evaluation')\n",
    "\n",
    "log_event(\n",
    "    event_name='micro_tutor_evaluation',\n",
    "    user_id=learner_id or session_id,\n",
    "    metadata={\n",
    "        'resource_id': resource_id,\n",
    "        'learner_role': learner_role,\n",
    "        'clarity': clarity,\n",
    "        'context': context,\n",
    "        'constraints': constraints,\n",
    "        'evaluation_score': evaluation_score,\n",
    "        'primary_weakness': primary_weakness,\n",
    "    },\n",
    ")\n",
    "\n",
    "print('\\n--- AI Response ---')\n",
    "print(ai_response)\n",
    "print('\\n--- Feedback ---')\n",
    "print(evaluation.get('summary', 'No summary provided.'))\n",
    "print('Primary weakness:', primary_weakness or 'n/a')\n",
    "print('Strengths:', ', '.join(evaluation.get('strengths', [])) or 'n/a')\n",
    "print('Suggestions:', ', '.join(evaluation.get('suggestions', [])) or 'n/a')\n",
    "print('\\nRecommended resource:', recommended_resource_id or 'n/a')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
