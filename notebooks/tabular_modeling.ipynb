{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33c0493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIRE Micro Tutor Identity Cell\n",
    "import datetime as dt\n",
    "learner_id = input('Enter your learner ID (email or handle): ').strip()\n",
    "learner_role = input('Enter your learner role (e.g., researcher, educator): ').strip()\n",
    "resource_id = 'notebook:tabular_modeling'\n",
    "session_id = f\"{learner_id}-{dt.datetime.utcnow().isoformat()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb093cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIRE Micro Tutor Imports\n",
    "from prompt_tutor import evaluate_prompt\n",
    "from aire_telemetry import log_event\n",
    "from resources_map import RESOURCE_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958fe92f",
   "metadata": {
    "id": "cell-f0fa7cab"
   },
   "outputs": [],
   "source": [
    "import sys, subprocess, os\n",
    "from pathlib import Path\n",
    "\n",
    "# Colab Setup\n",
    "if \"google.colab\" in sys.modules:\n",
    "    print(\"Running in Google Colab. Installing dependencies...\")\n",
    "    subprocess.run([\"pip\", \"install\", \"-q\", \"pandas\", \"numpy\", \"scikit-learn\", \"requests\", \"pydantic\", \"jsonschema\", \"plotly\", \"tqdm\"])\n",
    "    \n",
    "    # Check for data\n",
    "    if not (Path.cwd() / \"data\").exists():\n",
    "        print(\"Data directory not found. Cloning repository...\")\n",
    "        subprocess.run([\"git\", \"clone\", \"https://github.com/aire-program/aire-researcher-sandbox.git\", \"_repo\"])\n",
    "        \n",
    "        # Move data and scripts to current directory\n",
    "        if (Path(\"_repo/data\").exists()):\n",
    "            print(\"Moving data and scripts...\")\n",
    "            subprocess.run([\"mv\", \"_repo/data\", \".\"])\n",
    "            subprocess.run([\"mv\", \"_repo/scripts\", \".\"])\n",
    "            subprocess.run([\"rm\", \"-rf\", \"_repo\"])\n",
    "        else:\n",
    "            print(\"Warning: Data not found in cloned repo.\")\n",
    "    else:\n",
    "        print(\"Data directory found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dade1920",
   "metadata": {
    "id": "cell-8fe8f719"
   },
   "source": [
    "# Engineer Features from Experiments\n",
    "\n",
    "**What**: Derive new features from existing experimental data to enhance analytical power.\n",
    "\n",
    "**Why**: Raw data often lacks the specific signals needed for modeling. Feature engineering transforms raw variables into more meaningful representations.\n",
    "\n",
    "**How**:\n",
    "1. **Normalize metrics** (Z-score standardization).\n",
    "2. **Extract temporal features** (e.g., day of week).\n",
    "3. **Create binary indicators** for categorical variables.\n",
    "\n",
    "**Key Concept**: **Feature Engineering** is the process of using domain knowledge to extract features from raw data that make machine learning algorithms work better.\n",
    "\n",
    "By the end of this notebook, you will have completed the listed steps and produced the outputs described in the success criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8e3e0c",
   "metadata": {
    "id": "cell-78bff37d"
   },
   "source": [
    "### Success criteria\n",
    "- You computed normalized metrics.\n",
    "- You derived categorical/time features.\n",
    "- You prepared a feature-rich table for downstream use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cfdc8b",
   "metadata": {
    "id": "cell-65c6b718"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def find_data_dir() -> Path:\n",
    "    candidates = [Path.cwd() / \"data\", Path.cwd().parent / \"data\", Path.cwd().parent.parent / \"data\"]\n",
    "    for candidate in candidates:\n",
    "        if (candidate / \"sample_texts\" / \"articles_sample.csv\").exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError(\"data directory not found. Run scripts/generate_synthetic_data.py.\")\n",
    "\n",
    "DATA_DIR = find_data_dir()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa03ae4",
   "metadata": {
    "id": "cell-5e93775b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "experiments = pd.read_csv(DATA_DIR / \"sample_tabular\" / \"experiments_sample.csv\")\n",
    "experiments[\"timestamp\"] = pd.to_datetime(experiments[\"timestamp\"])\n",
    "\n",
    "experiments[\"metric_normalized\"] = (experiments[\"metric_value\"] - experiments[\"metric_value\"].mean()) / experiments[\"metric_value\"].std()\n",
    "experiments[\"day_of_week\"] = experiments[\"timestamp\"].dt.day_name()\n",
    "experiments[\"is_treatment\"] = experiments[\"condition\"].str.contains(\"treatment\")\n",
    "experiments[[\"experiment_id\", \"condition\", \"metric_value\", \"metric_normalized\", \"day_of_week\", \"is_treatment\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e33b5e",
   "metadata": {
    "id": "cell-e74abda2"
   },
   "source": [
    "### If you get stuck / What to try next\n",
    "\n",
    "If you get stuck: verify the cleaning notebook ran and that dependencies installed. What to try next: prototype simple models or export features to the notebook outputs for quick views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c1618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIRE Micro Tutor Prompt + Feedback Cell\n",
    "from aire_llm_client import chat_completion\n",
    "\n",
    "def call_llm(prompt_text: str) -> str:\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': 'You are an AI helper responding concisely.'},\n",
    "        {'role': 'user', 'content': prompt_text},\n",
    "    ]\n",
    "    return chat_completion(messages)\n",
    "\n",
    "prompt_text = input('Enter the prompt you want to test: ').strip()\n",
    "ai_response = call_llm(prompt_text)\n",
    "evaluation = evaluate_prompt(prompt_text, learner_role)\n",
    "\n",
    "clarity = evaluation.get('clarity_score')\n",
    "context = evaluation.get('context_score')\n",
    "constraints = evaluation.get('constraints_score')\n",
    "evaluation_score = evaluation.get('evaluation_score')\n",
    "primary_weakness = evaluation.get('primary_weakness', '')\n",
    "recommended_resource_id = RESOURCE_MAP.get(primary_weakness) or RESOURCE_MAP.get('evaluation')\n",
    "\n",
    "log_event(\n",
    "    event_name='micro_tutor_evaluation',\n",
    "    user_id=learner_id or session_id,\n",
    "    metadata={\n",
    "        'resource_id': resource_id,\n",
    "        'learner_role': learner_role,\n",
    "        'clarity': clarity,\n",
    "        'context': context,\n",
    "        'constraints': constraints,\n",
    "        'evaluation_score': evaluation_score,\n",
    "        'primary_weakness': primary_weakness,\n",
    "    },\n",
    ")\n",
    "\n",
    "print('\\n--- AI Response ---')\n",
    "print(ai_response)\n",
    "print('\\n--- Feedback ---')\n",
    "print(evaluation.get('summary', 'No summary provided.'))\n",
    "print('Primary weakness:', primary_weakness or 'n/a')\n",
    "print('Strengths:', ', '.join(evaluation.get('strengths', [])) or 'n/a')\n",
    "print('Suggestions:', ', '.join(evaluation.get('suggestions', [])) or 'n/a')\n",
    "print('\\nRecommended resource:', recommended_resource_id or 'n/a')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
