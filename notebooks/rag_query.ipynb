{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6781f036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIRE Micro Tutor Identity Cell\n",
    "import datetime as dt\n",
    "learner_id = input('Enter your learner ID (email or handle): ').strip()\n",
    "learner_role = input('Enter your learner role (e.g., researcher, educator): ').strip()\n",
    "resource_id = 'notebook:rag_query'\n",
    "session_id = f\"{learner_id}-{dt.datetime.utcnow().isoformat()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0044a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIRE Micro Tutor Imports\n",
    "from prompt_tutor import evaluate_prompt\n",
    "from aire_telemetry import log_event\n",
    "from resources_map import RESOURCE_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5651c2c8",
   "metadata": {
    "id": "cell-1d6e5eaf"
   },
   "outputs": [],
   "source": [
    "import sys, subprocess, os\n",
    "from pathlib import Path\n",
    "\n",
    "# Colab Setup\n",
    "if \"google.colab\" in sys.modules:\n",
    "    print(\"Running in Google Colab. Installing dependencies...\")\n",
    "    subprocess.run([\"pip\", \"install\", \"-q\", \"pandas\", \"numpy\", \"scikit-learn\", \"requests\", \"pydantic\", \"jsonschema\", \"plotly\", \"tqdm\"])\n",
    "    \n",
    "    # Check for data\n",
    "    if not (Path.cwd() / \"data\").exists():\n",
    "        print(\"Data directory not found. Cloning repository...\")\n",
    "        subprocess.run([\"git\", \"clone\", \"https://github.com/aire-program/aire-researcher-sandbox.git\", \"_repo\"])\n",
    "        \n",
    "        # Move data and scripts to current directory\n",
    "        if (Path(\"_repo/data\").exists()):\n",
    "            print(\"Moving data and scripts...\")\n",
    "            subprocess.run([\"mv\", \"_repo/data\", \".\"])\n",
    "            subprocess.run([\"mv\", \"_repo/scripts\", \".\"])\n",
    "            subprocess.run([\"rm\", \"-rf\", \"_repo\"])\n",
    "        else:\n",
    "            print(\"Warning: Data not found in cloned repo.\")\n",
    "    else:\n",
    "        print(\"Data directory found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dca459",
   "metadata": {
    "id": "cell-788a6b16"
   },
   "source": [
    "# Query the Retrieval Index\n",
    "\n",
    "**What**: Retrieve relevant documents from the pre-built TF-IDF index using natural language queries.\n",
    "\n",
    "**Why**: Retrieval is the core component of RAG (Retrieval-Augmented Generation) systems, allowing them to access external knowledge.\n",
    "\n",
    "**How**:\n",
    "1. **Load the saved index**.\n",
    "2. **Transform a user query** into a vector.\n",
    "3. **Calculate cosine similarity** to find the closest documents.\n",
    "\n",
    "**Key Concept**: **Cosine Similarity** measures the cosine of the angle between two vectors. A value close to 1 means the vectors (and thus the texts) are very similar.\n",
    "\n",
    "By the end of this notebook, you will have completed the listed steps and produced the outputs described in the success criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fdd660",
   "metadata": {
    "id": "cell-510a7755"
   },
   "source": [
    "### Success criteria\n",
    "- You loaded the TF-IDF index.\n",
    "- You ran at least one query.\n",
    "- You viewed a ranked list with scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18d3a1a",
   "metadata": {
    "id": "data-dir"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def find_data_dir() -> Path:\n",
    "    candidates = [Path.cwd() / \"data\", Path.cwd().parent / \"data\", Path.cwd().parent.parent / \"data\"]\n",
    "    for candidate in candidates:\n",
    "        if (candidate / \"sample_texts\" / \"articles_sample.csv\").exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError(\"data directory not found. Run scripts/generate_synthetic_data.py.\")\n",
    "\n",
    "\n",
    "DATA_DIR = find_data_dir()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cfdc8b",
   "metadata": {
    "id": "cell-5d76d7e1"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "articles = pd.read_csv(DATA_DIR / \"sample_texts\" / \"articles_sample.csv\")\n",
    "index_path = DATA_DIR / \"vector_index.pkl\"\n",
    "\n",
    "if not index_path.exists():\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    tfidf_matrix = vectorizer.fit_transform(articles[\"abstract\"].fillna(\"\"))\n",
    "    with open(index_path, \"wb\") as handle:\n",
    "        pickle.dump({\"vectorizer\": vectorizer, \"tfidf_matrix\": tfidf_matrix}, handle)\n",
    "else:\n",
    "    with open(index_path, \"rb\") as handle:\n",
    "        payload = pickle.load(handle)\n",
    "    vectorizer = payload[\"vectorizer\"]\n",
    "    tfidf_matrix = payload[\"tfidf_matrix\"]\n",
    "\n",
    "\n",
    "query = \"methods for reproducible ai\"\n",
    "query_vec = vectorizer.transform([query])\n",
    "scores = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "articles = articles.assign(score=scores)\n",
    "articles.sort_values(\"score\", ascending=False).head(5)[[\"title\", \"score\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e93f51",
   "metadata": {
    "id": "cell-f05e2cd8"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "articles = pd.read_csv(DATA_DIR / \"sample_texts\" / \"articles_sample.csv\")\n",
    "with open(DATA_DIR / \"vector_index.pkl\", \"rb\") as handle:\n",
    "    payload = pickle.load(handle)\n",
    "vectorizer = payload[\"vectorizer\"]\n",
    "tfidf_matrix = payload[\"tfidf_matrix\"]\n",
    "\n",
    "query = \"methods for reproducible ai\"\n",
    "query_vec = vectorizer.transform([query])\n",
    "scores = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "articles = articles.assign(score=scores)\n",
    "articles.sort_values(\"score\", ascending=False).head(5)[[\"title\", \"score\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410c2a4a",
   "metadata": {
    "id": "cell-27eb4f09"
   },
   "source": [
    "### If you get stuck / What to try next\n",
    "\n",
    "If you get stuck: rebuild the index or clear and rerun cells. What to try next: tune prompts in pipelines/rag/rag_evaluation.ipynb or integrate answers in pipelines/prototypes/minimal_research_assistant.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c8e587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIRE Micro Tutor Prompt + Feedback Cell\n",
    "from aire_llm_client import chat_completion\n",
    "\n",
    "def call_llm(prompt_text: str) -> str:\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': 'You are an AI helper responding concisely.'},\n",
    "        {'role': 'user', 'content': prompt_text},\n",
    "    ]\n",
    "    return chat_completion(messages)\n",
    "\n",
    "prompt_text = input('Enter the prompt you want to test: ').strip()\n",
    "ai_response = call_llm(prompt_text)\n",
    "evaluation = evaluate_prompt(prompt_text, learner_role)\n",
    "\n",
    "clarity = evaluation.get('clarity_score')\n",
    "context = evaluation.get('context_score')\n",
    "constraints = evaluation.get('constraints_score')\n",
    "evaluation_score = evaluation.get('evaluation_score')\n",
    "primary_weakness = evaluation.get('primary_weakness', '')\n",
    "recommended_resource_id = RESOURCE_MAP.get(primary_weakness) or RESOURCE_MAP.get('evaluation')\n",
    "\n",
    "log_event(\n",
    "    event_name='micro_tutor_evaluation',\n",
    "    user_id=learner_id or session_id,\n",
    "    metadata={\n",
    "        'resource_id': resource_id,\n",
    "        'learner_role': learner_role,\n",
    "        'clarity': clarity,\n",
    "        'context': context,\n",
    "        'constraints': constraints,\n",
    "        'evaluation_score': evaluation_score,\n",
    "        'primary_weakness': primary_weakness,\n",
    "    },\n",
    ")\n",
    "\n",
    "print('\\n--- AI Response ---')\n",
    "print(ai_response)\n",
    "print('\\n--- Feedback ---')\n",
    "print(evaluation.get('summary', 'No summary provided.'))\n",
    "print('Primary weakness:', primary_weakness or 'n/a')\n",
    "print('Strengths:', ', '.join(evaluation.get('strengths', [])) or 'n/a')\n",
    "print('Suggestions:', ', '.join(evaluation.get('suggestions', [])) or 'n/a')\n",
    "print('\\nRecommended resource:', recommended_resource_id or 'n/a')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
